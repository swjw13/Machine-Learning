{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "colab": {
      "name": "spamClassifier.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#전체 과정을 보여주는 코드 입니다. \n",
        "# 제출된 모델을 위한 코드를 따로 제출하였습니다."
      ],
      "metadata": {
        "id": "BHQFxx81xyFD"
      },
      "id": "BHQFxx81xyFD"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "95729ded"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import SVC\n",
        "import collections\n",
        "import random"
      ],
      "id": "95729ded",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ZKiQ4nTru3N"
      },
      "source": [
        "#Data preperation"
      ],
      "id": "5ZKiQ4nTru3N"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ed3b584a"
      },
      "source": [
        "whole_data = pd.read_csv(\"./data/train.csv\")\n",
        "test_data = pd.read_csv(\"./data/test.csv\")"
      ],
      "id": "ed3b584a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-CSb7QCQzW6"
      },
      "source": [
        "whole_data.head()"
      ],
      "id": "R-CSb7QCQzW6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1. model_feature_extraction(Logistic Regression, RaandomForest, SVM)\n"
      ],
      "metadata": {
        "id": "mSGb06RYM4ru"
      },
      "id": "mSGb06RYM4ru"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WnLwMvxgQ68L"
      },
      "source": [
        "##1. Data PreProcess\n"
      ],
      "id": "WnLwMvxgQ68L"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5_ib_JSiSZ8"
      },
      "source": [
        "whole_data['mail'] = whole_data['mail'].str.replace(\"Subject\", \"subject\")\n",
        "whole_data_nosubject = whole_data.copy()"
      ],
      "id": "g5_ib_JSiSZ8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVRUqgOVicc5"
      },
      "source": [
        "def subject_extraction(train):\n",
        "  subject_list = []\n",
        "  mail_list = []\n",
        "\n",
        "  for mail in train['mail']:\n",
        "    lst = mail.split(\"\\r\\n\")\n",
        "\n",
        "    find_subject = [i for i in range(len(lst)) if (\"subject:\" in lst[i]) or (\"subject :\" in lst[i])]\n",
        "\n",
        "    last_index = find_subject[-1]\n",
        "\n",
        "    if len(find_subject) > 1:\n",
        "      subject = []\n",
        "      mail = []\n",
        "      for i in range(find_subject[-1]+1):\n",
        "        subject.append(lst[i])\n",
        "      for i in range(find_subject[-1]+2,len(lst)):\n",
        "        mail.append(lst[i])\n",
        "      \n",
        "      subject_str = \"\"\n",
        "      mail_str = \"\"\n",
        "      for i in subject:\n",
        "        subject_str += i +\" \"\n",
        "      for i in mail:\n",
        "        mail_str += i + \" \"\n",
        "\n",
        "    else:\n",
        "      mail = []\n",
        "      for i in range(1,len(lst)):\n",
        "        mail.append(lst[i])\n",
        "      \n",
        "      mail_str = \"\"\n",
        "      for i in mail:\n",
        "        mail_str += i + \" \"\n",
        "      subject_str = str(lst[0])\n",
        "\n",
        "    subject_list.append(subject_str)\n",
        "    mail_list.append(mail_str)\n",
        "\n",
        "  return subject_list, mail_list"
      ],
      "id": "sVRUqgOVicc5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RzGJxSGxjAil"
      },
      "source": [
        "subject_list, mail_list = subject_extraction(whole_data)\n",
        "whole_data['subject'] = pd.DataFrame(subject_list)\n",
        "whole_data['mail'] = pd.DataFrame(mail_list)"
      ],
      "id": "RzGJxSGxjAil",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kxh5h1wDignd"
      },
      "source": [
        "whole_data.head()"
      ],
      "id": "Kxh5h1wDignd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0db698a"
      },
      "source": [
        "train_data, val_data = train_test_split(whole_data, test_size=0.2, random_state=42)"
      ],
      "id": "a0db698a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2e68d2d"
      },
      "source": [
        "print(len(val_data))\n",
        "print(len(train_data))"
      ],
      "id": "d2e68d2d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xT9rNTAaRNB4"
      },
      "source": [
        "train_data.head()"
      ],
      "id": "xT9rNTAaRNB4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2. FeatureExtraction"
      ],
      "metadata": {
        "id": "C-qs7FtqNS3z"
      },
      "id": "C-qs7FtqNS3z"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99131447"
      },
      "source": [
        "stopwords = ['i', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'herself',\n",
        "\"it\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were',\n",
        "'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at',\n",
        "'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over',\n",
        "'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such',\n",
        "'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now']"
      ],
      "id": "99131447",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T51KGfTL37gi"
      },
      "source": [
        "def remove_alphabet(data):\n",
        "  data = data.split()\n",
        "  result = []\n",
        "  for w in data:\n",
        "    if w not in [i for i in 'qwertyuiopasdfghjklzxcvbnm']:\n",
        "      result.append(w)\n",
        "  \n",
        "  return result\n",
        "\n",
        "def count_word_without_alphabet(data, top_count=10):\n",
        "    data = remove_alphabet(data)\n",
        "    result = []\n",
        "    for w in data:  \n",
        "        result.append(w) \n",
        "    \n",
        "    counts = collections.Counter(result)\n",
        "    return counts.most_common(top_count)\n",
        "\n",
        "def num_words_in_text(isSpam=1):\n",
        "  tmp = train_data.loc[train_data['label']==isSpam, 'mail'].str.replace('[^a-zA-Z]', ' ').map(count_word_without_alphabet)\n",
        "\n",
        "  word_count_list = {}\n",
        "\n",
        "  for index, item in enumerate(tmp):\n",
        "      for i in range(len(item)):\n",
        "          a = item[i][0]\n",
        "          if (a in word_count_list):\n",
        "              word_count_list[item[i][0]] += 1\n",
        "          else:\n",
        "              word_count_list[item[i][0]] = 1\n",
        "  \n",
        "  return word_count_list\n",
        "\n",
        "def is_word_in(train, word):\n",
        "    sr = train.str.contains(word).astype(int)\n",
        "    return sr"
      ],
      "id": "T51KGfTL37gi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d589d565"
      },
      "source": [
        "# spam 과 nonspam 내에서 단어의 빈도 수를 구함\n",
        "\n",
        "spam_word_list = num_words_in_text(1)\n",
        "non_spam_word_list = num_words_in_text(0)\n",
        "\n",
        "spam_list = sorted(spam_word_list.items(), key = (lambda x:x[1]), reverse = True)\n",
        "non_spam_list =  sorted(non_spam_word_list.items(), key = (lambda x:x[1]), reverse = True)\n",
        "\n",
        "for i in list(spam_list):\n",
        "  if i[0] not in stopwords and i[1] > 2:\n",
        "    print(i)"
      ],
      "id": "d589d565",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in list(non_spam_list):\n",
        "  if i[0] not in stopwords and i[1] > 2:\n",
        "    print(i)"
      ],
      "metadata": {
        "id": "BuvM3j2mWnlq"
      },
      "id": "BuvM3j2mWnlq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9ae594a"
      },
      "source": [
        "# spam과 nonspam 동시에 등장하는 단어 중 한 쪽 에서만 많이 등장 한 단어들을 정리\n",
        "\n",
        "duplicate_words = []\n",
        "\n",
        "for i in spam_word_list:\n",
        "    if (i in non_spam_word_list):\n",
        "        duplicate_words.append((i, spam_word_list[i], non_spam_word_list[i], spam_word_list[i] / non_spam_word_list[i]))\n",
        "\n",
        "duplicate_words_sorted = sorted(duplicate_words, key = (lambda x:x[3]), reverse = True)\n",
        "print(np.array(duplicate_words_sorted)[:30])\n",
        "\n",
        "duplicate_words_sorted = sorted(duplicate_words, key = (lambda x:x[3]), reverse = False)\n",
        "print(np.array(duplicate_words_sorted)[:30])"
      ],
      "id": "d9ae594a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75d00dbb"
      },
      "source": [
        "def make_feature(train):\n",
        "\n",
        "    def word_extraction(word_list, feature_object):\n",
        "        for w in word_list:\n",
        "            feature_object[w+'_in'] = is_word_in(train['mail'], w)\n",
        "        return feature_object\n",
        "    \n",
        "    email_len = train['mail'].str.len()\n",
        "    email_sub_len = train['subject'].str.len()\n",
        "    email_word_num = train['mail'].str.split().str.len()\n",
        "    email_sub_word_num = train['subject'].str.split().str.len()\n",
        "\n",
        "    features = pd.DataFrame()\n",
        "\n",
        "    features['sub_alphabet_num'] = train['subject'].str.replace('[^a-zA-Z]', '').str.len()\n",
        "    features['sub_alphabet_pro'] = train['subject'].str.replace('[^a-zA-Z]', '').str.len() / email_sub_len\n",
        "\n",
        "    features['sub_dec_num'] = train['subject'].str.replace('[^0-9]', '').str.len()\n",
        "    features['sub_dec_pro'] = train['subject'].str.replace('[^0-9]', '').str.len() / email_sub_len\n",
        "\n",
        "\n",
        "    features['exc_mark_num'] = train['mail'].str.replace('[^!]', '').str.len()\n",
        "    features['mail_dec_num'] = train['mail'].str.replace('[^0-9]', '').str.len()\n",
        "    features['mail_dec_pro'] = train['mail'].str.replace('[^0-9]', '').str.len() / email_len  \n",
        "\n",
        "\n",
        "    relative_words_nonspam = ['deal', 'gas', 'ls', 'will', 'any', 'there', 'day','contract','thanks','effective', 'month','flow','should', 'into','energy','may','please', 'week', 'suite', 'production', 'know','sale']\n",
        "    features['relative_nonspam_in'] = train['mail'].str.contains('|'.join(relative_words_nonspam)).astype(int)\n",
        "    features['relative_nonspam_num'] = train['mail'].str.count('|'.join(relative_words_nonspam))\n",
        "    features['relative_nonspam_pro'] = train['mail'].str.count('|'.join(relative_words_nonspam)) / email_word_num\n",
        "    features = word_extraction(relative_words_nonspam,features)\n",
        "\n",
        "\n",
        "    nonspam_most_words = ['enron', 'hpl','gas', 'deal', 'meter', 'attached', 'daren', 'xls', 'mmbtu','flow']\n",
        "    features['nonspam_in'] = train['mail'].str.contains('|'.join(nonspam_most_words)).astype(int)\n",
        "    features['nonspam_num'] = train['mail'].str.count('|'.join(nonspam_most_words))\n",
        "    features['nonspam_pro'] = train['mail'].str.count('|'.join(nonspam_most_words)) / email_word_num\n",
        "    features = word_extraction(nonspam_most_words,features) \n",
        "\n",
        "    \n",
        "    spam_most_words = ['http', 'com', 'www', 'new','get','want','online','viagra','hello','save','email','info','re','price','professional','penis', 'adobe', 'windows', 'pills','cheap', 'one', 'belize', 'net', 'image', 'cialis']\n",
        "    features['spam_in'] = train['mail'].str.contains('|'.join(spam_most_words)).astype(int)\n",
        "    features['spam_num'] = train['mail'].str.count('|'.join(spam_most_words))\n",
        "    features['spam_pro'] = train['mail'].str.count('|'.join(spam_most_words)) / email_word_num\n",
        "    features = word_extraction(spam_most_words,features)\n",
        "\n",
        "    relative_words_spam = ['www', 'online', 'best', 'money', 'save', 'http', 'hello','image','offer','more','over','us','only','com']\n",
        "    features['relative_spam_in'] = train['mail'].str.contains('|'.join(relative_words_spam)).astype(int)\n",
        "    features['relative_spam_num'] = train['mail'].str.count('|'.join(relative_words_spam))\n",
        "    features['relative_spam_pro'] = train['mail'].str.count('|'.join(relative_words_spam)) / email_word_num\n",
        "    features = word_extraction(relative_words_spam,features)\n",
        "\n",
        "    features['mail_len'] = train['mail'].str.len()\n",
        "\n",
        "    return features\n",
        "\n",
        "train_X = make_feature(train_data)\n",
        "withSpam = train_X.copy()\n",
        "withSpam['label'] = train_data['label']"
      ],
      "id": "75d00dbb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##(1) LogisticRegression"
      ],
      "metadata": {
        "id": "D5Mv1KeTN0kz"
      },
      "id": "D5Mv1KeTN0kz"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Izd9t77tM3lN"
      },
      "source": [
        "max_val = 0\n",
        "best_index_tmp = np.array([])\n",
        "feature_corr = withSpam.corr()['label'].sort_values().index\n",
        "feature_corr = feature_corr.drop(['label'])\n",
        "\n",
        "indexes = [i for i in range(len(feature_corr))]\n",
        "\n",
        "train_X = make_feature(train_data)\n",
        "val_X = make_feature(val_data)\n",
        "\n",
        "for i in range(1, len(feature_corr)):\n",
        "  for j in range(10):\n",
        "    model = LogisticRegression()\n",
        "    random_index = np.array(random.sample(indexes, i))\n",
        "    random_list = feature_corr[random_index]\n",
        "\n",
        "    my_model = model.fit(train_X[np.array(random_list)].fillna(0), train_data['label'])\n",
        "    score = my_model.score(val_X[random_list].fillna(0), val_data['label'])\n",
        "\n",
        "    if score > max_val:\n",
        "      best_index_tmp = random_list\n",
        "\n",
        "final_selected = best_index_tmp"
      ],
      "id": "Izd9t77tM3lN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_chosen_model = LogisticRegression()\n",
        "final_model = final_chosen_model.fit(train_X[final_selected].fillna(0), train_data['label'])\n",
        "\n",
        "prediction = final_model.predict(val_X[final_selected].fillna(0))\n",
        "label = val_data['label']\n",
        "acc = accuracy_score(prediction, label)\n",
        "prec = precision_score(prediction, label)\n",
        "recall = recall_score(prediction, label)\n",
        "f1 = f1_score(prediction, label)\n",
        "\n",
        "print(\"F1 score: {}, Accuracy: {}, precision: {}, recall: {}\".format(f1,acc,prec,recall))"
      ],
      "metadata": {
        "id": "qWNdTCUVOAcA"
      },
      "id": "qWNdTCUVOAcA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##(2) RandomForestClassifier"
      ],
      "metadata": {
        "id": "haJcpB45OB0g"
      },
      "id": "haJcpB45OB0g"
    },
    {
      "cell_type": "code",
      "source": [
        "max_val = 0\n",
        "best_index_tmp = np.array([])\n",
        "feature_corr = withSpam.corr()['label'].sort_values().index\n",
        "feature_corr = feature_corr.drop(['label'])\n",
        "\n",
        "indexes = [i for i in range(len(feature_corr))]\n",
        "\n",
        "train_X = make_feature(train_data)\n",
        "val_X = make_feature(val_data)\n",
        "\n",
        "for i in range(1, len(feature_corr)):\n",
        "  for j in range(10):\n",
        "    model = RandomForestClassifier()\n",
        "    random_index = np.array(random.sample(indexes, i))\n",
        "    random_list = feature_corr[random_index]\n",
        "\n",
        "    my_model = model.fit(train_X[np.array(random_list)].fillna(0), train_data['label'])\n",
        "    score = my_model.score(val_X[random_list].fillna(0), val_data['label'])\n",
        "\n",
        "    if score > max_val:\n",
        "      best_index_tmp = random_list\n",
        "\n",
        "final_selected = best_index_tmp"
      ],
      "metadata": {
        "id": "28FQDiELOEQd"
      },
      "id": "28FQDiELOEQd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_chosen_model = RandomForestClassifier()\n",
        "final_model = final_chosen_model.fit(train_X[final_selected].fillna(0), train_data['label'])\n",
        "\n",
        "prediction = final_model.predict(val_X[final_selected].fillna(0))\n",
        "label = val_data['label']\n",
        "acc = accuracy_score(prediction, label)\n",
        "prec = precision_score(prediction, label)\n",
        "recall = recall_score(prediction, label)\n",
        "f1 = f1_score(prediction, label)\n",
        "\n",
        "print(\"F1 score: {}, Accuracy: {}, precision: {}, recall: {}\".format(f1,acc,prec,recall))"
      ],
      "metadata": {
        "id": "DECciWfzOES6"
      },
      "id": "DECciWfzOES6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##(3) SVC"
      ],
      "metadata": {
        "id": "mwoynehvOGib"
      },
      "id": "mwoynehvOGib"
    },
    {
      "cell_type": "code",
      "source": [
        "max_val = 0\n",
        "best_index_tmp = np.array([])\n",
        "feature_corr = withSpam.corr()['label'].sort_values().index\n",
        "feature_corr = feature_corr.drop(['label'])\n",
        "\n",
        "indexes = [i for i in range(len(feature_corr))]\n",
        "\n",
        "train_X = make_feature(train_data)\n",
        "val_X = make_feature(val_data)\n",
        "\n",
        "for i in range(1, len(feature_corr)):\n",
        "  for j in range(10):\n",
        "    model = SVC()\n",
        "    random_index = np.array(random.sample(indexes, i))\n",
        "    random_list = feature_corr[random_index]\n",
        "\n",
        "    my_model = model.fit(train_X[np.array(random_list)].fillna(0), train_data['label'])\n",
        "    score = my_model.score(val_X[random_list].fillna(0), val_data['label'])\n",
        "\n",
        "    if score > max_val:\n",
        "      best_index_tmp = random_list\n",
        "\n",
        "final_selected = best_index_tmp"
      ],
      "metadata": {
        "id": "NQaqha1DOIsn"
      },
      "id": "NQaqha1DOIsn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_chosen_model = SVC()\n",
        "final_model = final_chosen_model.fit(train_X[final_selected].fillna(0), train_data['label'])\n",
        "\n",
        "prediction = final_model.predict(val_X[final_selected].fillna(0))\n",
        "label = val_data['label']\n",
        "acc = accuracy_score(prediction, label)\n",
        "prec = precision_score(prediction, label)\n",
        "recall = recall_score(prediction, label)\n",
        "f1 = f1_score(prediction, label)\n",
        "\n",
        "print(\"F1 score: {}, Accuracy: {}, precision: {}, recall: {}\".format(f1,acc,prec,recall))"
      ],
      "metadata": {
        "id": "qiRqobF_OIu8"
      },
      "id": "qiRqobF_OIu8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##(4) GaussianNB"
      ],
      "metadata": {
        "id": "5C7snZxkOKzI"
      },
      "id": "5C7snZxkOKzI"
    },
    {
      "cell_type": "code",
      "source": [
        "max_val = 0\n",
        "best_index_tmp = np.array([])\n",
        "feature_corr = withSpam.corr()['label'].sort_values().index\n",
        "feature_corr = feature_corr.drop(['label'])\n",
        "\n",
        "indexes = [i for i in range(len(feature_corr))]\n",
        "\n",
        "train_X = make_feature(train_data)\n",
        "val_X = make_feature(val_data)\n",
        "\n",
        "for i in range(1, len(feature_corr)):\n",
        "  for j in range(10):\n",
        "    model = GaussianNB()\n",
        "    random_index = np.array(random.sample(indexes, i))\n",
        "    random_list = feature_corr[random_index]\n",
        "\n",
        "    my_model = model.fit(train_X[np.array(random_list)].fillna(0), train_data['label'])\n",
        "    score = my_model.score(val_X[random_list].fillna(0), val_data['label'])\n",
        "\n",
        "    if score > max_val:\n",
        "      best_index_tmp = random_list\n",
        "\n",
        "final_selected = best_index_tmp"
      ],
      "metadata": {
        "id": "iIT-U_LROP4Z"
      },
      "id": "iIT-U_LROP4Z",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_chosen_model = GaussianNB()\n",
        "final_model = final_chosen_model.fit(train_X[final_selected].fillna(0), train_data['label'])\n",
        "\n",
        "prediction = final_model.predict(val_X[final_selected].fillna(0))\n",
        "label = val_data['label']\n",
        "acc = accuracy_score(prediction, label)\n",
        "prec = precision_score(prediction, label)\n",
        "recall = recall_score(prediction, label)\n",
        "f1 = f1_score(prediction, label)\n",
        "\n",
        "print(\"F1 score: {}, Accuracy: {}, precision: {}, recall: {}\".format(f1,acc,prec,recall))"
      ],
      "metadata": {
        "id": "1IjuWdO-OPgw"
      },
      "id": "1IjuWdO-OPgw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## randomforest with low features"
      ],
      "metadata": {
        "id": "OkoeCJz1ORUe"
      },
      "id": "OkoeCJz1ORUe"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vOTilZbgo9zD"
      },
      "source": [
        "selected_by_hand = ['nonspam_in','relative_nonspam_pro','nonspam_pro','sub_dec_pro','nonspam_num','relative_spam_pro','spam_pro','http_in','more_in','sub_alphabet_pro','exc_mark_num','relative_spam_num']"
      ],
      "id": "vOTilZbgo9zD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7BWaUsBYRqcH"
      },
      "source": [
        "final_chosen_model = RandomForestClassifier()\n",
        "final_model = final_chosen_model.fit(train_X[final_selected].fillna(0), train_data['label'])\n",
        "\n",
        "prediction = final_model.predict(val_X[final_selected].fillna(0))\n",
        "label = val_data['label']\n",
        "acc = accuracy_score(prediction, label)\n",
        "prec = precision_score(prediction, label)\n",
        "recall = recall_score(prediction, label)\n",
        "f1 = f1_score(prediction, label)\n",
        "\n",
        "print(\"F1 score: {}, Accuracy: {}, precision: {}, recall: {}\".format(f1,acc,prec,recall))"
      ],
      "id": "7BWaUsBYRqcH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRx8IY7MEyp2"
      },
      "source": [
        "test_data = pd.read_csv(\"./data/test.csv\")\n",
        "\n",
        "test_data['mail'] = test_data['mail'].str.replace(\"Subject\", \"subject\")\n",
        "subject_final, mail_final = subject_extraction(test_data)\n",
        "\n",
        "test_data['subject'] = pd.DataFrame(subject_final)\n",
        "test_data['mail'] = pd.DataFrame(mail_final)\n",
        "\n",
        "prediction = final_model.predict(make_feature(test_data)[final_selected].fillna(0))"
      ],
      "id": "ZRx8IY7MEyp2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQxh-OntBQdc"
      },
      "source": [
        "answer = test_data.copy()\n",
        "answer['label'] = np.array(prediction)\n",
        "del answer['mail']\n",
        "del answer['subject']\n",
        "\n",
        "import os\n",
        "\n",
        "outname = 'test.csv'\n",
        "\n",
        "outdir = './data'\n",
        "if not os.path.exists(outdir):\n",
        "    os.mkdir(outdir)\n",
        "\n",
        "fullname = os.path.join(outdir, outname)    \n",
        "\n",
        "answer.to_csv(fullname, index=False)"
      ],
      "id": "IQxh-OntBQdc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WkLMffpuefZ9"
      },
      "source": [
        "#2. RNN / LSTM"
      ],
      "id": "WkLMffpuefZ9"
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1. Data Preperation"
      ],
      "metadata": {
        "id": "b1XqLRlzObRp"
      },
      "id": "b1XqLRlzObRp"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jEdt51BuDQPQ"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.layers import Dense, SimpleRNN, LSTM, Embedding\n",
        "from keras import backend as K\n",
        "\n",
        "early_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)"
      ],
      "id": "jEdt51BuDQPQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def f1score(y_target, y_pred):\n",
        "    _recall = recall(y_target, y_pred)\n",
        "    _precision = precision(y_target, y_pred)\n",
        "    _f1score = ( 2 * _recall * _precision) / (_recall + _precision+ K.epsilon())\n",
        "\n",
        "    return _f1score\n",
        "    \n",
        "def recall(y_target, y_pred):\n",
        "    y_target_yn = K.round(K.clip(y_target, 0, 1))\n",
        "    y_pred_yn = K.round(K.clip(y_pred, 0, 1))\n",
        "\n",
        "    count_true_positive = K.sum(y_target_yn * y_pred_yn) \n",
        "\n",
        "    count_true_positive_false_negative = K.sum(y_target_yn)\n",
        "\n",
        "    recall = count_true_positive / (count_true_positive_false_negative + K.epsilon())\n",
        "\n",
        "    return recall\n",
        "\n",
        "def precision(y_target, y_pred):\n",
        "    y_pred_yn = K.round(K.clip(y_pred, 0, 1)) \n",
        "    y_target_yn = K.round(K.clip(y_target, 0, 1)) \n",
        "\n",
        "    count_true_positive = K.sum(y_target_yn * y_pred_yn) \n",
        "\n",
        "    count_true_positive_false_positive = K.sum(y_pred_yn)\n",
        "\n",
        "    precision = count_true_positive / (count_true_positive_false_positive + K.epsilon())\n",
        "\n",
        "    return precision"
      ],
      "metadata": {
        "id": "oKy1BxKBOz_v"
      },
      "id": "oKy1BxKBOz_v",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DVLq01fxEfo4"
      },
      "source": [
        "rnn_whole_data = pd.read_csv(\"./data/train.csv\")\n",
        "\n",
        "rnn_train_x, rnn_val_x = train_test_split(rnn_whole_data, test_size=0.2, random_state=42)\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(rnn_whole_data['mail'])\n",
        "\n",
        "X_train_encoded = tokenizer.texts_to_sequences(rnn_train_x['mail'])\n",
        "X_val_encoded = tokenizer.texts_to_sequences(rnn_val_x['mail'])\n",
        "\n",
        "max_len = max(len(l) for l in X_train_encoded+X_val_encoded)\n",
        "\n",
        "X_train_padded = pad_sequences(X_train_encoded, maxlen = max_len)\n",
        "X_val_padded = pad_sequences(X_val_encoded, maxlen = max_len)\n",
        "\n",
        "word_to_index = tokenizer.word_index\n",
        "vocab_size = len(word_to_index) + 1\n",
        "print(vocab_size)"
      ],
      "id": "DVLq01fxEfo4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOoX1CDuRb8B"
      },
      "source": [
        "##(1) RNN"
      ],
      "id": "zOoX1CDuRb8B"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48kMt4Po91or"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 64)) \n",
        "model.add(SimpleRNN(64))\n",
        "model.add(Dense(1, activation='tanh'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc', f1score, recall])\n",
        "\n",
        "history = model.fit(X_train_padded, rnn_train_x['label'], epochs=5, batch_size=32, validation_split=0.2, callbacks=[early_stop])"
      ],
      "id": "48kMt4Po91or",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WpuAUFF2jY6q"
      },
      "source": [
        "print(model.evaluate(X_val_padded, rnn_val_x['label'])[1])"
      ],
      "id": "WpuAUFF2jY6q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ujy0pv6pjnSp"
      },
      "source": [
        "epochs = range(1, len(history.history['acc']) + 1)\n",
        "plt.plot(epochs, history.history['loss'])\n",
        "plt.plot(epochs, history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()"
      ],
      "id": "Ujy0pv6pjnSp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qxc6CxBX_jLB"
      },
      "source": [
        "rnn_test_x = pd.read_csv(\"./data/test.csv\")\n",
        "X_test_encoded = tokenizer.texts_to_sequences(rnn_test_x['subject'] + rnn_test_x['mail'])\n",
        "X_test_padded = pad_sequences(X_test_encoded, maxlen = max_len)\n",
        "pred = model.predict(X_test_padded)"
      ],
      "id": "Qxc6CxBX_jLB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PIg33xW6ATnt"
      },
      "source": [
        "answer = test_data.copy()\n",
        "answer['label'] = np.round(pred).astype(np.int64)[:,0]\n",
        "del answer['mail']\n",
        "del answer['subject']\n",
        "\n",
        "import os\n",
        "\n",
        "outname = 'test.csv'\n",
        "\n",
        "outdir = './data'\n",
        "if not os.path.exists(outdir):\n",
        "    os.mkdir(outdir)\n",
        "\n",
        "fullname = os.path.join(outdir, outname)    \n",
        "\n",
        "answer.to_csv(fullname, index=False)"
      ],
      "id": "PIg33xW6ATnt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvLIZb3oDG0_"
      },
      "source": [
        "##(2) LSTM"
      ],
      "id": "RvLIZb3oDG0_"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ys4IXPe_DJE6"
      },
      "source": [
        "LSTMmodel = Sequential()\n",
        "LSTMmodel.add(Embedding(4000, 32))\n",
        "LSTMmodel.add(LSTM(32, dropout=0.1))\n",
        "LSTMmodel.add(Dense(1, activation='tanh'))\n",
        "\n",
        "LSTMmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc', f1score, recall])\n",
        "\n",
        "LSTMhistory = LSTMmodel.fit(X_train_padded, rnn_train_x['label'], epochs=10, batch_size=16, validation_split=0.2, callbacks=[early_stop])"
      ],
      "id": "Ys4IXPe_DJE6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L4AIs0QAESwm"
      },
      "source": [
        "print(LSTMmodel.evaluate(X_val_padded, rnn_val_x['label'])[1])"
      ],
      "id": "L4AIs0QAESwm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = range(1, len(LSTMhistory.history['acc']) + 1)\n",
        "plt.plot(epochs, LSTMhistory.history['loss'])\n",
        "plt.plot(epochs, LSTMhistory.history['val_loss'])\n",
        "plt.title('LSTMmodel loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EDpfYHjigUsR"
      },
      "id": "EDpfYHjigUsR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67krVjLnXCoA"
      },
      "source": [
        "X_test_encoded = tokenizer.texts_to_sequences(rnn_test_x['mail'])\n",
        "X_test_padded = pad_sequences(X_test_encoded, maxlen = max_len)\n",
        "pred = LSTMmodel.predict(X_test_padded)\n",
        "\n",
        "answer = rnn_test_x.copy()\n",
        "answer['label'] = np.round(pred).astype(np.int64)[:,0]\n",
        "del answer['mail']\n",
        "del answer['subject']\n",
        "\n",
        "import os\n",
        "\n",
        "outname = 'test.csv'\n",
        "\n",
        "outdir = './result'\n",
        "if not os.path.exists(outdir):\n",
        "    os.mkdir(outdir)\n",
        "\n",
        "fullname = os.path.join(outdir, outname)    \n",
        "\n",
        "answer.to_csv(fullname, index=False)"
      ],
      "id": "67krVjLnXCoA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFI1wVxeusRp"
      },
      "source": [
        "#3. Distilbert/Bert"
      ],
      "id": "PFI1wVxeusRp"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BKm111ryuvli"
      },
      "source": [
        "!pip install transformers"
      ],
      "id": "BKm111ryuvli",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s98g1nkFVGvI"
      },
      "source": [
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, confusion_matrix\n",
        "from transformers import Trainer, TrainingArguments\n",
        "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification\n",
        "from transformers import BertTokenizerFast, BertForSequenceClassification, BertTokenizer\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "id": "s98g1nkFVGvI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q7flUzY3v8WG"
      },
      "source": [
        "# 데이터셋 준비\n",
        "whole_data = pd.read_csv(\"./data/train.csv\")\n",
        "test_data = pd.read_csv(\"./data/test.csv\")\n",
        "\n",
        "\n",
        "bert_whole_data = whole_data.copy()\n",
        "bert_train_x, bert_val_x = train_test_split(bert_whole_data, test_size=0.2, random_state=42)\n",
        "bert_test_x = test_data.copy()"
      ],
      "id": "Q7flUzY3v8WG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AkAjjW5ozRjA"
      },
      "source": [
        "bert_train_list = []\n",
        "bert_train_label_list = []\n",
        "for i in bert_train_x['mail']:\n",
        "  bert_train_list.append(i)\n",
        "for i in bert_train_x['label']:\n",
        "  bert_train_label_list.append(i)\n",
        "\n",
        "\n",
        "bert_val_list = []\n",
        "bert_val_label_list = []\n",
        "for i in bert_val_x['mail']:\n",
        "  bert_val_list.append(i)\n",
        "for i in bert_val_x['label']:\n",
        "  bert_val_label_list.append(i)\n",
        "  \n",
        "  \n",
        "bert_test_list = []\n",
        "bert_test_label_list = []\n",
        "for i in bert_test_x['mail']:\n",
        "  bert_test_list.append(i)\n",
        "  bert_test_label_list.append(0)"
      ],
      "id": "AkAjjW5ozRjA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-8nDmf8SNhy"
      },
      "source": [
        "class Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "\n",
        "def compute_metrics(pred):\n",
        "  labels = pred.label_ids\n",
        "  preds = pred.predictions.argmax(-1)\n",
        "\n",
        "  acc = accuracy_score(labels, preds)\n",
        "  f1 = f1_score(labels, preds)\n",
        "  recall = recall_score(labels, preds)\n",
        "  precision = precision_score(labels, preds)\n",
        "\n",
        "  return { 'accuracy': acc, 'f1_score': f1, \"recall\": recall, \"precision_score\": precision }"
      ],
      "id": "j-8nDmf8SNhy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KqmOPPRBSYon"
      },
      "source": [
        "##(1) Distilbert"
      ],
      "id": "KqmOPPRBSYon"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPmStXinvrj_"
      },
      "source": [
        "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "train_encodings = tokenizer(bert_train_list, truncation=True, padding=True)\n",
        "val_encodings = tokenizer(bert_val_list, truncation=True, padding=True)\n",
        "test_encodings = tokenizer(bert_test_list, truncation=True, padding=True)\n",
        "\n",
        "train_dataset = Dataset(train_encodings, bert_train_label_list)\n",
        "val_dataset = Dataset(val_encodings, bert_val_label_list)\n",
        "test_dataset = Dataset(test_encodings, bert_test_label_list)"
      ],
      "id": "aPmStXinvrj_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H8uZBZJlSiyV"
      },
      "source": [
        "model = BertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2).to(\"cuda\")\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=32,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    load_best_model_at_end=True,\n",
        "    logging_steps=100,\n",
        "    evaluation_strategy=\"steps\",\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ],
      "id": "H8uZBZJlSiyV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WVMpb-pv9C_r"
      },
      "source": [
        "trainer.train()"
      ],
      "id": "WVMpb-pv9C_r",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oMODuCjWGKs2"
      },
      "source": [
        "result = trainer.predict(test_dataset)\n",
        "preds = []\n",
        "\n",
        "for i in result[0]:\n",
        "  if i[0] > i[1]:\n",
        "    preds.append(0)\n",
        "  else:\n",
        "    preds.append(1)\n",
        "\n",
        "label = bert_test_x.copy()\n",
        "\n",
        "label['label'] = pd.DataFrame(preds)\n",
        "del label['mail']\n",
        "\n",
        "import os\n",
        "\n",
        "outname = 'test.csv'\n",
        "\n",
        "outdir = './result'\n",
        "if not os.path.exists(outdir):\n",
        "    os.mkdir(outdir)\n",
        "\n",
        "fullname = os.path.join(outdir, outname)    \n",
        "\n",
        "label.to_csv(fullname, index=False)"
      ],
      "id": "oMODuCjWGKs2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3slzEiSmFyGq"
      },
      "source": [
        "#(2)distilbert with kfold"
      ],
      "id": "3slzEiSmFyGq"
    },
    {
      "cell_type": "code",
      "source": [
        "model = BertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2).to(\"cuda\")\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "kf = KFold(n_splits=5, random_state=99, shuffle=True)\n",
        "X = np.array(bert_train_list+bert_val_list)\n",
        "y = np.array(bert_train_label_list+bert_val_label_list)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=32,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    load_best_model_at_end=True,\n",
        "    logging_steps=100,\n",
        "    evaluation_strategy=\"steps\",\n",
        ")"
      ],
      "metadata": {
        "id": "6ygzAM--RGHN"
      },
      "id": "6ygzAM--RGHN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for train_index, test_index in kf.split(X, y):\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "    x_list = []\n",
        "    y_list = []\n",
        "    for i in X_train:\n",
        "      x_list.append(i)\n",
        "    for i in y_train:\n",
        "      y_list.append(i)\n",
        "\n",
        "    train_encoded = tokenizer(x_list, truncation=True, padding=True)\n",
        "    train_data = Dataset(train_encoded, y_list)\n",
        "\n",
        "    x_list = []\n",
        "    y_list = []\n",
        "    for i in X_test:\n",
        "      x_list.append(i)\n",
        "    for i in y_test:\n",
        "      y_list.append(i)\n",
        "\n",
        "    val_encoded = tokenizer(x_list, truncation=True, padding=True)\n",
        "    val_data = Dataset(val_encoded, y_list)\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_data,\n",
        "        eval_dataset=val_data,\n",
        "        compute_metrics=compute_metrics,\n",
        "    )\n",
        "\n",
        "    trainer.train()"
      ],
      "metadata": {
        "id": "poEAy19-RGE5"
      },
      "id": "poEAy19-RGE5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_encodings = tokenizer(bert_test_list, truncation=True, padding=True)\n",
        "test_dataset = Dataset(test_encodings, bert_test_label_list)\n",
        "\n",
        "result = trainer.predict(test_dataset)\n",
        "preds = []\n",
        "\n",
        "for i in result[0]:\n",
        "  if i[0] > i[1]:\n",
        "    preds.append(0)\n",
        "  else:\n",
        "    preds.append(1)\n",
        "\n",
        "label = bert_test_x.copy()\n",
        "\n",
        "label['label'] = pd.DataFrame(preds)\n",
        "del label['mail']\n",
        "\n",
        "import os\n",
        "\n",
        "outname = 'test.csv'\n",
        "\n",
        "outdir = './result'\n",
        "if not os.path.exists(outdir):\n",
        "    os.mkdir(outdir)\n",
        "\n",
        "fullname = os.path.join(outdir, outname)    \n",
        "\n",
        "label.to_csv(fullname, index=False)"
      ],
      "metadata": {
        "id": "I14kG74yRGCW"
      },
      "id": "I14kG74yRGCW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTz6gGKARU7k"
      },
      "source": [
        "##(3) Bert"
      ],
      "id": "OTz6gGKARU7k"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5-vOw-jQEYD"
      },
      "source": [
        "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\", do_lower_case=True)\n",
        "\n",
        "train_encodings = tokenizer(bert_train_list, truncation=True, padding=True)\n",
        "val_encodings = tokenizer(bert_val_list, truncation=True, padding=True)\n",
        "test_encodings = tokenizer(bert_test_list, truncation=True, padding=True)\n",
        "\n",
        "train_dataset = Dataset(train_encodings, bert_train_label_list)\n",
        "val_dataset = Dataset(val_encodings, bert_val_label_list)\n",
        "test_dataset = Dataset(test_encodings, bert_test_label_list)"
      ],
      "id": "H5-vOw-jQEYD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_exr7FGQjHj"
      },
      "source": [
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2).to(\"cuda\")\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=32,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    load_best_model_at_end=True,\n",
        "    logging_steps=100,\n",
        "    evaluation_strategy=\"steps\",\n",
        ")"
      ],
      "id": "w_exr7FGQjHj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6u8UTJ8gQ12S"
      },
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "id": "6u8UTJ8gQ12S",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FKScyRX7URAg"
      },
      "source": [
        "result = trainer.predict(test_dataset)\n",
        "answers = []\n",
        "for i in result[0]:\n",
        "  if i[0] > i[1]:\n",
        "    answers.append(0)\n",
        "  else:\n",
        "    answers.append(1)\n",
        "answer = bert_test_x.copy()\n",
        "answer['label'] = pd.DataFrame(answers)\n",
        "del answer['mail']\n",
        "\n",
        "import os\n",
        "\n",
        "outname = 'test.csv'\n",
        "\n",
        "outdir = './result'\n",
        "if not os.path.exists(outdir):\n",
        "    os.mkdir(outdir)\n",
        "\n",
        "fullname = os.path.join(outdir, outname)    \n",
        "\n",
        "answer.to_csv(fullname, index=False)"
      ],
      "id": "FKScyRX7URAg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezkhXluyUZK-"
      },
      "source": [
        "##(4) bert with kfold"
      ],
      "id": "ezkhXluyUZK-"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecPS8LHLWBdG"
      },
      "source": [
        "kf = KFold(n_splits=5, random_state=99, shuffle=True)\n",
        "X = np.array(bert_train_list+bert_val_list)\n",
        "y = np.array(bert_train_label_list+bert_val_label_list)\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2).to(\"cuda\")\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", do_lower_case=True)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=32,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    load_best_model_at_end=True,\n",
        "    logging_steps=100,\n",
        "    evaluation_strategy=\"steps\",\n",
        ")"
      ],
      "id": "ecPS8LHLWBdG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R8PYOadNUY1-"
      },
      "source": [
        "for train_index, test_index in kf.split(X, y):\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "    x_list = []\n",
        "    y_list = []\n",
        "    for i in X_train:\n",
        "      x_list.append(i)\n",
        "    for i in y_train:\n",
        "      y_list.append(i)\n",
        "\n",
        "    train_encoded = tokenizer(x_list, truncation=True, padding=True)\n",
        "    train_data = Dataset(train_encoded, y_list)\n",
        "\n",
        "    x_list = []\n",
        "    y_list = []\n",
        "    for i in X_test:\n",
        "      x_list.append(i)\n",
        "    for i in y_test:\n",
        "      y_list.append(i)\n",
        "\n",
        "    test_encoded = tokenizer(x_list, truncation=True, padding=True)\n",
        "    test_data = Dataset(test_encoded, y_list)\n",
        "\n",
        "    trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_data,\n",
        "    eval_dataset=test_data,\n",
        "    compute_metrics=compute_metrics,\n",
        "    )\n",
        "\n",
        "    trainer.train()"
      ],
      "id": "R8PYOadNUY1-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DNqK90PnWII2"
      },
      "source": [
        "result = trainer.predict(test_dataset)\n",
        "answers = []\n",
        "for i in result[0]:\n",
        "  if i[0] > i[1]:\n",
        "    answers.append(0)\n",
        "  else:\n",
        "    answers.append(1)\n",
        "answer = bert_test_x.copy()\n",
        "answer['label'] = pd.DataFrame(answers)\n",
        "del answer['mail']\n",
        "\n",
        "import os\n",
        "\n",
        "outname = 'test.csv'\n",
        "\n",
        "outdir = './result'\n",
        "if not os.path.exists(outdir):\n",
        "    os.mkdir(outdir)\n",
        "\n",
        "fullname = os.path.join(outdir, outname)    \n",
        "\n",
        "answer.to_csv(fullname, index=False)"
      ],
      "id": "DNqK90PnWII2",
      "execution_count": null,
      "outputs": []
    }
  ]
}